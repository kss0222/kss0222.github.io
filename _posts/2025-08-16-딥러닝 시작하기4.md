---
layout: post
title:  "자연어처리 개요"
date:   2025-08-16
author: SeongShin.K
categories: 자연어처리및실습
tags: NLP 딥러닝
---

<b>자연어처리(Natural Language Processing, NLP)란?</b>  

자연어처리(NLP)는 사람이 일상적으로 사용하는 언어(한국어, 영어 등)를 컴퓨터가 이해하고 활용할 수 있도록 하는 인공지능 기술입니다.  
대표적인 응용 분야로는 **스팸 메일 분류, 기계 번역, 챗봇, 감성 분석, 질의응답 시스템** 등이 있습니다.  

<img src="/assets/nlp_overview2.png" width="300"/>  

---
## 1. NLP 기본 아이디어
문장을 컴퓨터가 처리하려면, 먼저 **문장을 숫자로 바꾸는 과정**이 필요합니다.  
흐름은 다음과 같습니다.

<b>문장 → 토큰화(Tokenization) → 벡터화(Vectorization) → 분류/생성</b>

예시:  
- "나는 밥을 먹었다" → ["나는", "밥", "먹었다"] → [0,1,0,1,0...] → 분류기 입력  

---
## 2. Bag-of-Words (단어 주머니) 방법
가장 기본적인 텍스트 표현 방법은 **Bag-of-Words**입니다.  
문장에서 단어의 등장 여부(빈도)를 세어 행렬로 표현합니다.

```python

from sklearn.feature_extraction.text import CountVectorizer

docs = [“광고 문자: 이번 주말 한정 세일!”,
        “교수님, 과제 파일 올렸습니다.”,
        “무료 쿠폰 받으세요, 지금 클릭!”]
cv = CountVectorizer() X = cv.fit_transform(docs)
print(“Vocabulary:”, cv.vocabulary_) print(“Bag-of-Words Matrix:\n”, X.toarray())
```python
```

## 3. TF-IDF (단어 중요도 반영)

Bag-of-Words는 단순 빈도 기반이라서,
문서 전체에서 자주 등장하는 **불필요한 단어(예: "그리고")**에도 높은 점수를 줄 수 있습니다.

이를 보완하는 기법이 TF-IDF (Term Frequency – Inverse Document Frequency) 입니다.

TF: 특정 문서 안에서 단어의 빈도

IDF: 전체 문서에서 흔히 등장하는 단어일수록 가중치 ↓

즉, 문서마다 의미 있는 단어에 더 큰 가중치를 줍니다.

## 4. 단어 임베딩 (Word Embedding)

TF-IDF까지는 단어를 "숫자"로만 다룹니다.
하지만 언어의 의미적 관계를 반영하기 위해선, 비슷한 단어는 비슷한 벡터로 표현해야 합니다.

이를 가능하게 한 기술이 Word2Vec, GloVe 입니다.

예시:

왕 – 남자 + 여자 ≈ 여왕


➡️ 단어 간 의미 관계를 수학적으로 모델링할 수 있습니다.

## 5. 딥러닝과 Transformer

최근에는 단어 단위 벡터를 넘어서 **문맥(Context)**까지 반영합니다.

RNN/LSTM: 순서가 있는 문장을 처리 (시간 흐름 반영)

Transformer: 문장의 모든 단어가 서로에게 주의를 기울이게(Self-Attention)

대표 모델:

<b>BERT</b>: 문맥을 양방향으로 이해 → 분류, 질의응답

<b>GPT</b>: 문맥을 이어가며 자연스러운 문장 생성 → 챗봇, 글쓰기
