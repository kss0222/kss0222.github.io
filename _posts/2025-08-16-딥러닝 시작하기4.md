---
layout: post
title:  "자연어처리 시작하기 1"
date:   2025-08-16
author: SeongShin.K
categories: 자연어처리및실습
tags: NLP, 딥러닝
---

<b>자연어처리(Natural Language Processing, NLP)란?</b>  

자연어처리(NLP)는 사람이 일상적으로 사용하는 언어(한국어, 영어 등)를 컴퓨터가 이해하고 활용할 수 있도록 하는 인공지능 기술입니다.  
대표적인 응용 분야로는 **스팸 메일 분류, 기계 번역, 챗봇, 감성 분석, 질의응답 시스템** 등이 있습니다.  

<img src="/assets/nlp_overview2.png" width="200"/>  

---

## 1. NLP 기본 아이디어
문장을 컴퓨터가 처리하려면, 먼저 **문장을 숫자로 바꾸는 과정**이 필요합니다.  
흐름은 다음과 같습니다.

<b>문장 → 토큰화(Tokenization) → 벡터화(Vectorization) → 분류/생성</b>

예시:  
- "나는 밥을 먹었다" → ["나는", "밥", "먹었다"] → [0,1,0,1,0...] → 분류기 입력  

---

## 2. Bag-of-Words (단어 주머니) 방법
가장 기본적인 텍스트 표현 방법은 **Bag-of-Words**입니다.  
문장에서 단어의 등장 여부(빈도)를 세어 행렬로 표현합니다.

```python
from sklearn.feature_extraction.text import CountVectorizer

docs = ["광고 문자: 이번 주말 한정 세일!",
        "교수님, 과제 파일 올렸습니다.",
        "무료 쿠폰 받으세요, 지금 클릭!"]

cv = CountVectorizer()
X = cv.fit_transform(docs)

print("Vocabulary:", cv.vocabulary_)
print("Bag-of-Words Matrix:\n", X.toarray())

##3. TF-IDF (단어 중요도 반영)

Bag-of-Words는 단어의 단순 빈도만 세기 때문에, 문서 전체에서 자주 등장하는 불필요한 단어(예: “그리고”, “하지만”)에도 높은 점수를 줄 수 있다는 한계가 있습니다.

이를 보완한 방법이 **TF-IDF (Term Frequency – Inverse Document Frequency)**입니다.

TF (단어 빈도): 특정 문서 안에서 단어가 얼마나 자주 등장하는지

IDF (역문서 빈도): 전체 문서에서 흔히 등장하는 단어일수록 가중치를 낮게 주는 것

즉, 문서마다 의미 있는 단어에 더 큰 가중치를 주어 분석할 수 있습니다.

##4. 단어 임베딩(Word Embedding)

TF-IDF까지는 여전히 단어를 “숫자”로만 다룹니다. 하지만 언어의 의미를 제대로 반영하기 위해선, 비슷한 단어는 비슷한 벡터로 표현할 수 있어야 합니다.

이를 가능하게 한 기술이 Word2Vec, GloVe 같은 단어 임베딩 기법입니다.
예를 들어,

“왕 – 남자 + 여자 = 여왕” 같은 연산이 벡터 공간에서 가능합니다.
즉, 단어 간의 의미적 관계를 수학적으로 모델링할 수 있는 것이죠.

##5. 딥러닝과 트랜스포머(Transformer)

최근에는 단어 단위 벡터를 넘어서, **문맥(Context)**까지 반영하는 방식이 발전했습니다.

RNN/LSTM: 순서가 있는 문장을 처리하는데 강점

Transformer: 문장의 모든 단어가 서로에게 주의를 기울일 수 있게(Self-Attention) 설계

대표적인 모델이 BERT와 GPT입니다.

BERT: 문맥을 양방향으로 이해 → 문장 분류, 질의응답 등에 강점

GPT: 문맥을 이어가며 자연스러운 문장 생성 → 챗봇, 글쓰기 보조 등에 활용
